apiVersion: gateway.envoyproxy.io/v1alpha1
kind: EnvoyProxy
metadata:
  name: ai-gateway-telemetry
  namespace: envoy-gateway-system
spec:
  telemetry:
    metrics:
      prometheus:
        disable: false
    accessLog:
      settings:
      - format:
          type: Text
          text: |
            [%START_TIME%] "%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%"
            %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT%
            %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% "%REQ(X-FORWARDED-FOR)%"
            "%REQ(USER-AGENT)%" "%REQ(X-REQUEST-ID)%" "%REQ(:AUTHORITY)%" "%UPSTREAM_HOST%"
            tenant="%REQ(X-TENANT)%" model="%REQ(X-MODEL)%" jwt_sub="%REQ(X-JWT-SUB)%"
            tokens_in="%DYNAMIC_METADATA(io.envoy.ai_gateway:llm_input_token)%" 
            tokens_out="%DYNAMIC_METADATA(io.envoy.ai_gateway:llm_output_token)%"
            tokens_total="%DYNAMIC_METADATA(io.envoy.ai_gateway:llm_total_token)%"
        sinks:
        - type: File
          file:
            path: /tmp/access.log
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ai-gateway-dashboard
  namespace: monitoring
data:
  dashboard.json: |
    {
      "dashboard": {
        "title": "AI Gateway Performance Dashboard",
        "panels": [
          {
            "title": "Model Request Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(ai_model_requests_total[5m])",
                "legendFormat": "{{tenant}}-{{model}}"
              }
            ]
          },
          {
            "title": "Model Response Time",
            "type": "graph",
            "targets": [
              {
                "expr": "histogram_quantile(0.95, ai_model_request_duration_seconds_bucket)",
                "legendFormat": "95th percentile"
              }
            ]
          },
          {
            "title": "Token Usage",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(ai_model_tokens_processed[5m])",
                "legendFormat": "Input Tokens"
              },
              {
                "expr": "rate(ai_model_tokens_generated[5m])",
                "legendFormat": "Output Tokens"
              }
            ]
          },
          {
            "title": "Error Rate by Tenant",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(ai_model_requests_total{response_code!~\"2..\"}[5m])",
                "legendFormat": "{{tenant}} errors"
              }
            ]
          }
        ]
      }
    }